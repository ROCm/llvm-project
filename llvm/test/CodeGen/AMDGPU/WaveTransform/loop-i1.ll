; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx90a -mattr=+wavefrontsize64 -verify-machineinstrs -stop-after=finalize-isel -amdgpu-wave-transform-cf=1 < %s | FileCheck -check-prefixes=GFX90a %s
; RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1200 -verify-machineinstrs -stop-after=finalize-isel -amdgpu-wave-transform-cf=1 < %s | FileCheck -check-prefixes=GFX1200 %s
; RUN: llc -global-isel -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1200 -verify-machineinstrs -stop-after=finalize-isel -amdgpu-wave-transform-cf=0 < %s | FileCheck -check-prefixes=GISEL %s

; __global__ void loop_i1(int *filter,  int *out) {
;   int id = __builtin_amdgcn_workitem_id_x();
;   bool sel = false;
;   if (id > 5) {
;     int x;
;     do {
;       x = filter[id];
;       sel = sel ^ (x < 7);
;       id += 64;
;     } while (x > 11);
;   } else
;     sel = (filter[id] > 2);
;   if (sel)
;     out[id] = id;
; }

define amdgpu_kernel void @loop_i1(ptr addrspace(1) %filter.coerce, ptr addrspace(1) %out.coerce) {
  ; GFX90a-LABEL: name: loop_i1
  ; GFX90a: bb.0.entry:
  ; GFX90a-NEXT:   successors: %bb.1(0x40000000), %bb.3(0x40000000)
  ; GFX90a-NEXT:   liveins: $vgpr0, $sgpr8_sgpr9
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT:   [[COPY:%[0-9]+]]:sgpr_64(p4) = COPY $sgpr8_sgpr9
  ; GFX90a-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32(s32) = COPY $vgpr0
  ; GFX90a-NEXT:   [[S_LOAD_DWORDX4_IMM:%[0-9]+]]:sgpr_128 = S_LOAD_DWORDX4_IMM [[COPY]](p4), 0, 0 :: (dereferenceable invariant load (s128) from %ir.filter.coerce.kernarg.offset2, addrspace 4)
  ; GFX90a-NEXT:   [[COPY2:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub1
  ; GFX90a-NEXT:   [[COPY3:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub0
  ; GFX90a-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:sreg_64 = REG_SEQUENCE killed [[COPY3]], %subreg.sub0, killed [[COPY2]], %subreg.sub1
  ; GFX90a-NEXT:   [[COPY4:%[0-9]+]]:sreg_64_xexec_xnull = COPY [[REG_SEQUENCE]]
  ; GFX90a-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 1023
  ; GFX90a-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 [[COPY1]](s32), killed [[S_MOV_B32_]], implicit $exec
  ; GFX90a-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32 = S_MOV_B32 6
  ; GFX90a-NEXT:   [[V_CMP_LT_U32_e64_:%[0-9]+]]:sreg_64 = V_CMP_LT_U32_e64 [[V_AND_B32_e64_]], killed [[S_MOV_B32_1]], implicit $exec
  ; GFX90a-NEXT:   SI_BRCOND killed [[V_CMP_LT_U32_e64_]], %bb.3
  ; GFX90a-NEXT:   S_BRANCH %bb.1
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT: bb.1.do.body.preheader:
  ; GFX90a-NEXT:   successors: %bb.2(0x80000000)
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT:   [[S_MOV_B32_2:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GFX90a-NEXT:   [[V_LSHLREV_B32_e64_:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 killed [[S_MOV_B32_2]], [[V_AND_B32_e64_]], implicit $exec
  ; GFX90a-NEXT:   [[S_MOV_B32_3:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; GFX90a-NEXT:   [[DEF:%[0-9]+]]:sgpr_32 = IMPLICIT_DEF
  ; GFX90a-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; GFX90a-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64_align2 = REG_SEQUENCE [[V_LSHLREV_B32_e64_]], %subreg.sub0, killed [[V_MOV_B32_e32_]], %subreg.sub1
  ; GFX90a-NEXT:   [[COPY5:%[0-9]+]]:sreg_32_xm0_xexec = COPY [[COPY4]].sub0
  ; GFX90a-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY [[REG_SEQUENCE1]].sub0
  ; GFX90a-NEXT:   [[COPY7:%[0-9]+]]:sreg_32_xm0_xexec = COPY [[COPY4]].sub1
  ; GFX90a-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY [[REG_SEQUENCE1]].sub1
  ; GFX90a-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 [[COPY5]], [[COPY6]], 0, implicit $exec
  ; GFX90a-NEXT:   [[COPY9:%[0-9]+]]:vgpr_32 = COPY [[COPY7]]
  ; GFX90a-NEXT:   [[V_ADDC_U32_e64_:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY9]], [[COPY8]], killed [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
  ; GFX90a-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64_align2 = REG_SEQUENCE [[V_ADD_CO_U32_e64_]], %subreg.sub0, [[V_ADDC_U32_e64_]], %subreg.sub1
  ; GFX90a-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_MOV_B64 0
  ; GFX90a-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, [[S_MOV_B64_]], implicit $exec
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT: bb.2.do.body:
  ; GFX90a-NEXT:   successors: %bb.2(0x7c000000), %bb.4(0x04000000)
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT:   [[PHI:%[0-9]+]]:vreg_64_align2 = PHI [[REG_SEQUENCE2]], %bb.1, %9, %bb.2
  ; GFX90a-NEXT:   [[PHI1:%[0-9]+]]:vgpr_32 = PHI [[V_CNDMASK_B32_e64_]], %bb.1, %66, %bb.2
  ; GFX90a-NEXT:   [[PHI2:%[0-9]+]]:vgpr_32 = PHI [[V_AND_B32_e64_]], %bb.1, %8, %bb.2
  ; GFX90a-NEXT:   [[GLOBAL_LOAD_DWORD:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[PHI]], 0, 0, implicit $exec :: (load (s32) from %ir.lsr.iv, addrspace 1)
  ; GFX90a-NEXT:   [[S_MOV_B32_4:%[0-9]+]]:sreg_32 = S_MOV_B32 7
  ; GFX90a-NEXT:   [[V_CMP_LT_I32_e64_:%[0-9]+]]:sreg_64 = V_CMP_LT_I32_e64 [[GLOBAL_LOAD_DWORD]], killed [[S_MOV_B32_4]], implicit $exec
  ; GFX90a-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64 = V_CMP_NE_U32_e64 [[PHI1]], 0, implicit $exec
  ; GFX90a-NEXT:   [[S_XOR_B64_:%[0-9]+]]:sreg_64_xexec = S_XOR_B64 [[V_CMP_NE_U32_e64_]], killed [[V_CMP_LT_I32_e64_]], implicit-def dead $scc
  ; GFX90a-NEXT:   [[V_CNDMASK_B32_e64_1:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, [[S_XOR_B64_]], implicit $exec
  ; GFX90a-NEXT:   [[S_MOV_B32_5:%[0-9]+]]:sreg_32 = S_MOV_B32 64
  ; GFX90a-NEXT:   [[V_ADD_U32_e64_:%[0-9]+]]:vgpr_32 = nuw V_ADD_U32_e64 [[PHI2]], killed [[S_MOV_B32_5]], 0, implicit $exec
  ; GFX90a-NEXT:   [[S_MOV_B32_6:%[0-9]+]]:sreg_32 = S_MOV_B32 11
  ; GFX90a-NEXT:   [[V_CMP_GT_I32_e64_:%[0-9]+]]:sreg_64 = V_CMP_GT_I32_e64 [[GLOBAL_LOAD_DWORD]], killed [[S_MOV_B32_6]], implicit $exec
  ; GFX90a-NEXT:   [[S_MOV_B:%[0-9]+]]:sreg_64 = S_MOV_B64_IMM_PSEUDO 256
  ; GFX90a-NEXT:   [[COPY10:%[0-9]+]]:vgpr_32 = COPY [[PHI]].sub0
  ; GFX90a-NEXT:   [[COPY11:%[0-9]+]]:sreg_32_xm0 = COPY [[S_MOV_B]].sub0
  ; GFX90a-NEXT:   [[COPY12:%[0-9]+]]:vgpr_32 = COPY [[PHI]].sub1
  ; GFX90a-NEXT:   [[COPY13:%[0-9]+]]:sreg_32_xm0 = COPY [[S_MOV_B]].sub1
  ; GFX90a-NEXT:   [[V_ADD_CO_U32_e64_2:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_3:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 [[COPY10]], [[COPY11]], 0, implicit $exec
  ; GFX90a-NEXT:   [[COPY14:%[0-9]+]]:vgpr_32 = COPY [[COPY13]]
  ; GFX90a-NEXT:   [[V_ADDC_U32_e64_2:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_3:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY12]], [[COPY14]], killed [[V_ADD_CO_U32_e64_3]], 0, implicit $exec
  ; GFX90a-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64_align2 = REG_SEQUENCE [[V_ADD_CO_U32_e64_2]], %subreg.sub0, [[V_ADDC_U32_e64_2]], %subreg.sub1
  ; GFX90a-NEXT:   SI_BRCOND killed [[V_CMP_GT_I32_e64_]], %bb.2
  ; GFX90a-NEXT:   S_BRANCH %bb.4
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT: bb.3.if.else:
  ; GFX90a-NEXT:   successors: %bb.4(0x80000000)
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT:   [[S_MOV_B32_7:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GFX90a-NEXT:   [[V_LSHLREV_B32_e64_1:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 [[S_MOV_B32_7]], [[V_AND_B32_e64_]], implicit $exec
  ; GFX90a-NEXT:   [[GLOBAL_LOAD_DWORD_SADDR:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR [[COPY4]], killed [[V_LSHLREV_B32_e64_1]], 0, 0, implicit $exec :: (load (s32) from %ir.arrayidx7, addrspace 1)
  ; GFX90a-NEXT:   [[V_CMP_GT_I32_e64_1:%[0-9]+]]:sreg_64_xexec = V_CMP_GT_I32_e64 killed [[GLOBAL_LOAD_DWORD_SADDR]], [[S_MOV_B32_7]], implicit $exec
  ; GFX90a-NEXT:   [[V_CNDMASK_B32_e64_2:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, [[V_CMP_GT_I32_e64_1]], implicit $exec
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT: bb.4.if.end:
  ; GFX90a-NEXT:   successors: %bb.5(0x40000000), %bb.6(0x40000000)
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT:   [[PHI3:%[0-9]+]]:vgpr_32 = PHI [[V_CNDMASK_B32_e64_2]], %bb.3, [[V_CNDMASK_B32_e64_1]], %bb.2
  ; GFX90a-NEXT:   [[PHI4:%[0-9]+]]:vgpr_32 = PHI [[V_AND_B32_e64_]], %bb.3, [[V_ADD_U32_e64_]], %bb.2
  ; GFX90a-NEXT:   [[V_CMP_NE_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 [[PHI3]], 0, implicit $exec
  ; GFX90a-NEXT:   [[V_CNDMASK_B32_e64_3:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, 1, [[V_CMP_NE_U32_e64_1]], implicit $exec
  ; GFX90a-NEXT:   [[S_MOV_B32_8:%[0-9]+]]:sreg_32 = S_MOV_B32 1
  ; GFX90a-NEXT:   [[V_CMP_NE_U32_e64_2:%[0-9]+]]:sreg_64 = V_CMP_NE_U32_e64 killed [[V_CNDMASK_B32_e64_3]], killed [[S_MOV_B32_8]], implicit $exec
  ; GFX90a-NEXT:   SI_BRCOND killed [[V_CMP_NE_U32_e64_2]], %bb.6
  ; GFX90a-NEXT:   S_BRANCH %bb.5
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT: bb.5.if.then11:
  ; GFX90a-NEXT:   successors: %bb.6(0x80000000)
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT:   [[COPY15:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub3
  ; GFX90a-NEXT:   [[COPY16:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub2
  ; GFX90a-NEXT:   [[REG_SEQUENCE4:%[0-9]+]]:sreg_64 = REG_SEQUENCE killed [[COPY16]], %subreg.sub0, killed [[COPY15]], %subreg.sub1
  ; GFX90a-NEXT:   [[V_ASHRREV_I32_e64_:%[0-9]+]]:vgpr_32 = V_ASHRREV_I32_e64 31, [[PHI4]], implicit $exec
  ; GFX90a-NEXT:   [[COPY17:%[0-9]+]]:vgpr_32 = COPY [[V_ASHRREV_I32_e64_]]
  ; GFX90a-NEXT:   [[REG_SEQUENCE5:%[0-9]+]]:vreg_64_align2 = REG_SEQUENCE [[PHI4]], %subreg.sub0, killed [[COPY17]], %subreg.sub1
  ; GFX90a-NEXT:   [[S_MOV_B32_9:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GFX90a-NEXT:   [[V_LSHLREV_B64_e64_:%[0-9]+]]:vreg_64_align2 = nsw V_LSHLREV_B64_e64 killed [[S_MOV_B32_9]], killed [[REG_SEQUENCE5]], implicit $exec
  ; GFX90a-NEXT:   [[COPY18:%[0-9]+]]:sreg_32_xexec_hi_and_sreg_32_xm0 = COPY [[REG_SEQUENCE4]].sub0
  ; GFX90a-NEXT:   [[COPY19:%[0-9]+]]:vgpr_32 = COPY [[V_LSHLREV_B64_e64_]].sub0
  ; GFX90a-NEXT:   [[COPY20:%[0-9]+]]:sreg_32_xexec_hi_and_sreg_32_xm0 = COPY [[REG_SEQUENCE4]].sub1
  ; GFX90a-NEXT:   [[COPY21:%[0-9]+]]:vgpr_32 = COPY [[V_LSHLREV_B64_e64_]].sub1
  ; GFX90a-NEXT:   [[V_ADD_CO_U32_e64_4:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_5:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 [[COPY18]], [[COPY19]], 0, implicit $exec
  ; GFX90a-NEXT:   [[COPY22:%[0-9]+]]:vgpr_32 = COPY [[COPY20]]
  ; GFX90a-NEXT:   [[V_ADDC_U32_e64_4:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_5:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY22]], [[COPY21]], killed [[V_ADD_CO_U32_e64_5]], 0, implicit $exec
  ; GFX90a-NEXT:   [[REG_SEQUENCE6:%[0-9]+]]:vreg_64_align2 = REG_SEQUENCE [[V_ADD_CO_U32_e64_4]], %subreg.sub0, [[V_ADDC_U32_e64_4]], %subreg.sub1
  ; GFX90a-NEXT:   GLOBAL_STORE_DWORD killed [[REG_SEQUENCE6]], [[PHI4]], 0, 0, implicit $exec :: (store (s32) into %ir.arrayidx13, addrspace 1)
  ; GFX90a-NEXT: {{  $}}
  ; GFX90a-NEXT: bb.6.if.end14:
  ; GFX90a-NEXT:   S_ENDPGM 0
  ;
  ; GFX1200-LABEL: name: loop_i1
  ; GFX1200: bb.0.entry:
  ; GFX1200-NEXT:   successors: %bb.1(0x40000000), %bb.3(0x40000000)
  ; GFX1200-NEXT:   liveins: $vgpr0, $sgpr4_sgpr5
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[COPY:%[0-9]+]]:sgpr_64(p4) = COPY $sgpr4_sgpr5
  ; GFX1200-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32(s32) = COPY $vgpr0
  ; GFX1200-NEXT:   [[S_LOAD_DWORDX4_IMM:%[0-9]+]]:sgpr_128 = S_LOAD_DWORDX4_IMM [[COPY]](p4), 0, 0 :: (dereferenceable invariant load (s128) from %ir.filter.coerce.kernarg.offset2, addrspace 4)
  ; GFX1200-NEXT:   [[COPY2:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub1
  ; GFX1200-NEXT:   [[COPY3:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub0
  ; GFX1200-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:sreg_64 = REG_SEQUENCE killed [[COPY3]], %subreg.sub0, killed [[COPY2]], %subreg.sub1
  ; GFX1200-NEXT:   [[COPY4:%[0-9]+]]:sreg_64_xexec_xnull = COPY [[REG_SEQUENCE]]
  ; GFX1200-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 1023
  ; GFX1200-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 [[COPY1]](s32), killed [[S_MOV_B32_]], implicit $exec
  ; GFX1200-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32 = S_MOV_B32 6
  ; GFX1200-NEXT:   [[V_CMP_LT_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_LT_U32_e64 [[V_AND_B32_e64_]], killed [[S_MOV_B32_1]], implicit $exec
  ; GFX1200-NEXT:   SI_BRCOND killed [[V_CMP_LT_U32_e64_]], %bb.3
  ; GFX1200-NEXT:   S_BRANCH %bb.1
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.1.do.body.preheader:
  ; GFX1200-NEXT:   successors: %bb.2(0x80000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[S_MOV_B32_2:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GFX1200-NEXT:   [[V_LSHLREV_B32_e64_:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 killed [[S_MOV_B32_2]], [[V_AND_B32_e64_]], implicit $exec
  ; GFX1200-NEXT:   [[S_MOV_B32_3:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; GFX1200-NEXT:   [[DEF:%[0-9]+]]:sgpr_32 = IMPLICIT_DEF
  ; GFX1200-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; GFX1200-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_LSHLREV_B32_e64_]], %subreg.sub0, killed [[V_MOV_B32_e32_]], %subreg.sub1
  ; GFX1200-NEXT:   [[COPY5:%[0-9]+]]:sreg_32_xm0_xexec = COPY [[COPY4]].sub0
  ; GFX1200-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY [[REG_SEQUENCE1]].sub0
  ; GFX1200-NEXT:   [[COPY7:%[0-9]+]]:sreg_32_xm0_xexec = COPY [[COPY4]].sub1
  ; GFX1200-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY [[REG_SEQUENCE1]].sub1
  ; GFX1200-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[COPY5]], [[COPY6]], 0, implicit $exec
  ; GFX1200-NEXT:   [[V_ADDC_U32_e64_:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADDC_U32_e64 [[COPY7]], [[COPY8]], killed [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
  ; GFX1200-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_]], %subreg.sub0, [[V_ADDC_U32_e64_]], %subreg.sub1
  ; GFX1200-NEXT:   [[S_MOV_B32_4:%[0-9]+]]:sreg_32_xm0_xexec = S_MOV_B32 0
  ; GFX1200-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, [[S_MOV_B32_4]], implicit $exec
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.2.do.body:
  ; GFX1200-NEXT:   successors: %bb.2(0x7c000000), %bb.4(0x04000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[PHI:%[0-9]+]]:vreg_64 = PHI [[REG_SEQUENCE2]], %bb.1, %9, %bb.2
  ; GFX1200-NEXT:   [[PHI1:%[0-9]+]]:vgpr_32 = PHI [[V_CNDMASK_B32_e64_]], %bb.1, %61, %bb.2
  ; GFX1200-NEXT:   [[PHI2:%[0-9]+]]:vgpr_32 = PHI [[V_AND_B32_e64_]], %bb.1, %8, %bb.2
  ; GFX1200-NEXT:   [[GLOBAL_LOAD_DWORD:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[PHI]], 0, 0, implicit $exec :: (load (s32) from %ir.lsr.iv, addrspace 1)
  ; GFX1200-NEXT:   [[S_MOV_B32_5:%[0-9]+]]:sreg_32 = S_MOV_B32 7
  ; GFX1200-NEXT:   [[V_CMP_LT_I32_e64_:%[0-9]+]]:sreg_32 = V_CMP_LT_I32_e64 [[GLOBAL_LOAD_DWORD]], killed [[S_MOV_B32_5]], implicit $exec
  ; GFX1200-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_NE_U32_e64 [[PHI1]], 0, implicit $exec
  ; GFX1200-NEXT:   [[S_XOR_B32_:%[0-9]+]]:sreg_32_xm0_xexec = S_XOR_B32 [[V_CMP_NE_U32_e64_]], killed [[V_CMP_LT_I32_e64_]], implicit-def dead $scc
  ; GFX1200-NEXT:   [[V_CNDMASK_B32_e64_1:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, [[S_XOR_B32_]], implicit $exec
  ; GFX1200-NEXT:   [[S_MOV_B32_6:%[0-9]+]]:sreg_32 = S_MOV_B32 64
  ; GFX1200-NEXT:   [[V_ADD_U32_e64_:%[0-9]+]]:vgpr_32 = nuw V_ADD_U32_e64 [[PHI2]], killed [[S_MOV_B32_6]], 0, implicit $exec
  ; GFX1200-NEXT:   [[S_MOV_B32_7:%[0-9]+]]:sreg_32 = S_MOV_B32 11
  ; GFX1200-NEXT:   [[V_CMP_GT_I32_e64_:%[0-9]+]]:sreg_32 = V_CMP_GT_I32_e64 [[GLOBAL_LOAD_DWORD]], killed [[S_MOV_B32_7]], implicit $exec
  ; GFX1200-NEXT:   [[S_MOV_B:%[0-9]+]]:sreg_64 = S_MOV_B64_IMM_PSEUDO 256
  ; GFX1200-NEXT:   [[COPY9:%[0-9]+]]:vgpr_32 = COPY [[PHI]].sub0
  ; GFX1200-NEXT:   [[COPY10:%[0-9]+]]:sreg_32_xm0 = COPY [[S_MOV_B]].sub0
  ; GFX1200-NEXT:   [[COPY11:%[0-9]+]]:vgpr_32 = COPY [[PHI]].sub1
  ; GFX1200-NEXT:   [[COPY12:%[0-9]+]]:sreg_32_xm0 = COPY [[S_MOV_B]].sub1
  ; GFX1200-NEXT:   [[V_ADD_CO_U32_e64_2:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_3:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[COPY9]], [[COPY10]], 0, implicit $exec
  ; GFX1200-NEXT:   [[V_ADDC_U32_e64_2:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_3:%[0-9]+]]:sreg_32_xm0_xexec = V_ADDC_U32_e64 [[COPY11]], [[COPY12]], killed [[V_ADD_CO_U32_e64_3]], 0, implicit $exec
  ; GFX1200-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_2]], %subreg.sub0, [[V_ADDC_U32_e64_2]], %subreg.sub1
  ; GFX1200-NEXT:   SI_BRCOND killed [[V_CMP_GT_I32_e64_]], %bb.2
  ; GFX1200-NEXT:   S_BRANCH %bb.4
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.3.if.else:
  ; GFX1200-NEXT:   successors: %bb.4(0x80000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[S_MOV_B32_8:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GFX1200-NEXT:   [[V_LSHLREV_B32_e64_1:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 [[S_MOV_B32_8]], [[V_AND_B32_e64_]], implicit $exec
  ; GFX1200-NEXT:   [[GLOBAL_LOAD_DWORD_SADDR:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR [[COPY4]], killed [[V_LSHLREV_B32_e64_1]], 0, 0, implicit $exec :: (load (s32) from %ir.arrayidx7, addrspace 1)
  ; GFX1200-NEXT:   [[V_CMP_GT_I32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GT_I32_e64 killed [[GLOBAL_LOAD_DWORD_SADDR]], [[S_MOV_B32_8]], implicit $exec
  ; GFX1200-NEXT:   [[V_CNDMASK_B32_e64_2:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, [[V_CMP_GT_I32_e64_1]], implicit $exec
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.4.if.end:
  ; GFX1200-NEXT:   successors: %bb.5(0x40000000), %bb.6(0x40000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[PHI3:%[0-9]+]]:vgpr_32 = PHI [[V_CNDMASK_B32_e64_2]], %bb.3, [[V_CNDMASK_B32_e64_1]], %bb.2
  ; GFX1200-NEXT:   [[PHI4:%[0-9]+]]:vgpr_32 = PHI [[V_AND_B32_e64_]], %bb.3, [[V_ADD_U32_e64_]], %bb.2
  ; GFX1200-NEXT:   [[V_CMP_NE_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_NE_U32_e64 [[PHI3]], 0, implicit $exec
  ; GFX1200-NEXT:   [[V_CNDMASK_B32_e64_3:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, 1, [[V_CMP_NE_U32_e64_1]], implicit $exec
  ; GFX1200-NEXT:   [[S_MOV_B32_9:%[0-9]+]]:sreg_32 = S_MOV_B32 1
  ; GFX1200-NEXT:   [[V_CMP_NE_U32_e64_2:%[0-9]+]]:sreg_32 = V_CMP_NE_U32_e64 killed [[V_CNDMASK_B32_e64_3]], killed [[S_MOV_B32_9]], implicit $exec
  ; GFX1200-NEXT:   SI_BRCOND killed [[V_CMP_NE_U32_e64_2]], %bb.6
  ; GFX1200-NEXT:   S_BRANCH %bb.5
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.5.if.then11:
  ; GFX1200-NEXT:   successors: %bb.6(0x80000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[COPY13:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub3
  ; GFX1200-NEXT:   [[COPY14:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub2
  ; GFX1200-NEXT:   [[REG_SEQUENCE4:%[0-9]+]]:sreg_64 = REG_SEQUENCE killed [[COPY14]], %subreg.sub0, killed [[COPY13]], %subreg.sub1
  ; GFX1200-NEXT:   [[V_ASHRREV_I32_e64_:%[0-9]+]]:vgpr_32 = V_ASHRREV_I32_e64 31, [[PHI4]], implicit $exec
  ; GFX1200-NEXT:   [[COPY15:%[0-9]+]]:vgpr_32 = COPY [[V_ASHRREV_I32_e64_]]
  ; GFX1200-NEXT:   [[REG_SEQUENCE5:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI4]], %subreg.sub0, killed [[COPY15]], %subreg.sub1
  ; GFX1200-NEXT:   [[S_MOV_B32_10:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GFX1200-NEXT:   [[V_LSHLREV_B64_pseudo_e64_:%[0-9]+]]:vreg_64 = nsw V_LSHLREV_B64_pseudo_e64 killed [[S_MOV_B32_10]], killed [[REG_SEQUENCE5]], implicit $exec
  ; GFX1200-NEXT:   [[COPY16:%[0-9]+]]:sreg_32_xexec_hi_and_sreg_32_xm0 = COPY [[REG_SEQUENCE4]].sub0
  ; GFX1200-NEXT:   [[COPY17:%[0-9]+]]:vgpr_32 = COPY [[V_LSHLREV_B64_pseudo_e64_]].sub0
  ; GFX1200-NEXT:   [[COPY18:%[0-9]+]]:sreg_32_xexec_hi_and_sreg_32_xm0 = COPY [[REG_SEQUENCE4]].sub1
  ; GFX1200-NEXT:   [[COPY19:%[0-9]+]]:vgpr_32 = COPY [[V_LSHLREV_B64_pseudo_e64_]].sub1
  ; GFX1200-NEXT:   [[V_ADD_CO_U32_e64_4:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_5:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[COPY16]], [[COPY17]], 0, implicit $exec
  ; GFX1200-NEXT:   [[V_ADDC_U32_e64_4:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_5:%[0-9]+]]:sreg_32_xm0_xexec = V_ADDC_U32_e64 [[COPY18]], [[COPY19]], killed [[V_ADD_CO_U32_e64_5]], 0, implicit $exec
  ; GFX1200-NEXT:   [[REG_SEQUENCE6:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_4]], %subreg.sub0, [[V_ADDC_U32_e64_4]], %subreg.sub1
  ; GFX1200-NEXT:   GLOBAL_STORE_DWORD killed [[REG_SEQUENCE6]], [[PHI4]], 0, 0, implicit $exec :: (store (s32) into %ir.arrayidx13, addrspace 1)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.6.if.end14:
  ; GFX1200-NEXT:   S_ENDPGM 0
  ;
  ; GISEL-LABEL: name: loop_i1
  ; GISEL: bb.1.entry:
  ; GISEL-NEXT:   successors: %bb.6(0x40000000), %bb.2(0x40000000)
  ; GISEL-NEXT:   liveins: $sgpr4_sgpr5, $vgpr0
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[COPY:%[0-9]+]]:sreg_64 = COPY $sgpr4_sgpr5
  ; GISEL-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; GISEL-NEXT:   [[DEF:%[0-9]+]]:sreg_32 = IMPLICIT_DEF
  ; GISEL-NEXT:   [[S_LOAD_DWORDX4_IMM:%[0-9]+]]:sgpr_128 = S_LOAD_DWORDX4_IMM [[COPY]], 0, 0 :: (dereferenceable invariant load (<2 x s64>) from %ir.filter.coerce.kernarg.offset2, addrspace 4)
  ; GISEL-NEXT:   [[COPY2:%[0-9]+]]:sreg_64 = COPY [[S_LOAD_DWORDX4_IMM]].sub0_sub1
  ; GISEL-NEXT:   [[COPY3:%[0-9]+]]:sreg_64 = COPY [[S_LOAD_DWORDX4_IMM]].sub2_sub3
  ; GISEL-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 1023
  ; GISEL-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_]]
  ; GISEL-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 [[COPY1]], [[COPY4]], implicit $exec
  ; GISEL-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32 = S_MOV_B32 5
  ; GISEL-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_1]]
  ; GISEL-NEXT:   [[V_CMP_LE_U32_e64_:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_LE_U32_e64 [[V_AND_B32_e64_]], [[COPY5]], implicit $exec
  ; GISEL-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32 = S_AND_B32 1, [[DEF]], implicit-def dead $scc
  ; GISEL-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_NE_U32_e64 0, [[S_AND_B32_]], implicit $exec
  ; GISEL-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32_xm0_xexec = SI_IF [[V_CMP_LE_U32_e64_]], %bb.2, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_BRANCH %bb.6
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.2.Flow5:
  ; GISEL-NEXT:   successors: %bb.3(0x40000000), %bb.4(0x40000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[PHI:%[0-9]+]]:sreg_32 = PHI [[V_CMP_NE_U32_e64_]], %bb.1, %99, %bb.6
  ; GISEL-NEXT:   [[COPY6:%[0-9]+]]:sreg_32_xm0_xexec = COPY [[PHI]]
  ; GISEL-NEXT:   [[SI_ELSE:%[0-9]+]]:sreg_32_xm0_xexec = SI_ELSE [[SI_IF]], %bb.4, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_BRANCH %bb.3
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.3.do.body.preheader:
  ; GISEL-NEXT:   successors: %bb.5(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[S_MOV_B32_2:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GISEL-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_2]]
  ; GISEL-NEXT:   [[V_LSHLREV_B32_e64_:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 [[COPY7]], [[V_AND_B32_e64_]], implicit $exec
  ; GISEL-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; GISEL-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_LSHLREV_B32_e64_]], %subreg.sub0, [[V_MOV_B32_e32_]], %subreg.sub1
  ; GISEL-NEXT:   [[COPY8:%[0-9]+]]:vreg_64 = COPY [[COPY2]]
  ; GISEL-NEXT:   [[COPY9:%[0-9]+]]:vgpr_32 = COPY [[COPY8]].sub0
  ; GISEL-NEXT:   [[COPY10:%[0-9]+]]:vgpr_32 = COPY [[REG_SEQUENCE]].sub0
  ; GISEL-NEXT:   [[COPY11:%[0-9]+]]:vgpr_32 = COPY [[COPY8]].sub1
  ; GISEL-NEXT:   [[COPY12:%[0-9]+]]:vgpr_32 = COPY [[REG_SEQUENCE]].sub1
  ; GISEL-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[COPY9]], [[COPY10]], 0, implicit $exec
  ; GISEL-NEXT:   [[V_ADDC_U32_e64_:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADDC_U32_e64 [[COPY11]], [[COPY12]], killed [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
  ; GISEL-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_]], %subreg.sub0, [[V_ADDC_U32_e64_]], %subreg.sub1
  ; GISEL-NEXT:   [[S_MOV_B32_3:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; GISEL-NEXT:   [[DEF1:%[0-9]+]]:sreg_32 = IMPLICIT_DEF
  ; GISEL-NEXT:   [[S_MOV_B32_4:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; GISEL-NEXT:   [[DEF2:%[0-9]+]]:sreg_32 = IMPLICIT_DEF
  ; GISEL-NEXT:   S_BRANCH %bb.5
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.4.Flow6:
  ; GISEL-NEXT:   successors: %bb.8(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[PHI1:%[0-9]+]]:sreg_32_xm0_xexec = PHI [[PHI]], %bb.2, %106, %bb.7
  ; GISEL-NEXT:   [[PHI2:%[0-9]+]]:vgpr_32 = PHI %50, %bb.7, [[V_AND_B32_e64_]], %bb.2
  ; GISEL-NEXT:   SI_END_CF [[SI_ELSE]], implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_BRANCH %bb.8
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.5.do.body:
  ; GISEL-NEXT:   successors: %bb.7(0x04000000), %bb.5(0x7c000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[PHI3:%[0-9]+]]:sreg_32 = PHI [[DEF2]], %bb.3, %116, %bb.5
  ; GISEL-NEXT:   [[PHI4:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_4]], %bb.3, %113, %bb.5
  ; GISEL-NEXT:   [[PHI5:%[0-9]+]]:sreg_32 = PHI [[DEF1]], %bb.3, %91, %bb.5
  ; GISEL-NEXT:   [[PHI6:%[0-9]+]]:sreg_32_xm0_xexec = PHI [[S_MOV_B32_3]], %bb.3, %47, %bb.5
  ; GISEL-NEXT:   [[PHI7:%[0-9]+]]:vreg_64 = PHI %46, %bb.5, [[REG_SEQUENCE1]], %bb.3
  ; GISEL-NEXT:   [[PHI8:%[0-9]+]]:vgpr_32 = PHI %42, %bb.5, [[V_AND_B32_e64_]], %bb.3
  ; GISEL-NEXT:   [[GLOBAL_LOAD_DWORD:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[PHI7]], 0, 0, implicit $exec :: (load (s32) from %ir.lsr.iv, addrspace 1)
  ; GISEL-NEXT:   [[S_MOV_B32_5:%[0-9]+]]:sreg_32 = S_MOV_B32 7
  ; GISEL-NEXT:   [[COPY13:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_5]]
  ; GISEL-NEXT:   [[V_CMP_LT_I32_e64_:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_LT_I32_e64 [[GLOBAL_LOAD_DWORD]], [[COPY13]], implicit $exec
  ; GISEL-NEXT:   [[S_XOR_B32_:%[0-9]+]]:sreg_32_xm0_xexec = S_XOR_B32 [[PHI4]], [[V_CMP_LT_I32_e64_]], implicit-def dead $scc
  ; GISEL-NEXT:   [[COPY14:%[0-9]+]]:sreg_32 = COPY [[S_XOR_B32_]]
  ; GISEL-NEXT:   [[S_MOV_B32_6:%[0-9]+]]:sreg_32 = S_MOV_B32 64
  ; GISEL-NEXT:   [[COPY15:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_6]]
  ; GISEL-NEXT:   [[V_ADD_U32_e64_:%[0-9]+]]:vgpr_32 = nuw V_ADD_U32_e64 [[PHI8]], [[COPY15]], 0, implicit $exec
  ; GISEL-NEXT:   [[S_MOV_B32_7:%[0-9]+]]:sreg_32 = S_MOV_B32 11
  ; GISEL-NEXT:   [[COPY16:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_7]]
  ; GISEL-NEXT:   [[V_CMP_LE_I32_e64_:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_LE_I32_e64 [[GLOBAL_LOAD_DWORD]], [[COPY16]], implicit $exec
  ; GISEL-NEXT:   [[S_MOV_B:%[0-9]+]]:sreg_64 = S_MOV_B64_IMM_PSEUDO 256
  ; GISEL-NEXT:   [[COPY17:%[0-9]+]]:vreg_64 = COPY [[S_MOV_B]]
  ; GISEL-NEXT:   [[COPY18:%[0-9]+]]:vgpr_32 = COPY [[PHI7]].sub0
  ; GISEL-NEXT:   [[COPY19:%[0-9]+]]:vgpr_32 = COPY [[COPY17]].sub0
  ; GISEL-NEXT:   [[COPY20:%[0-9]+]]:vgpr_32 = COPY [[PHI7]].sub1
  ; GISEL-NEXT:   [[COPY21:%[0-9]+]]:vgpr_32 = COPY [[COPY17]].sub1
  ; GISEL-NEXT:   [[V_ADD_CO_U32_e64_2:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_3:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[COPY18]], [[COPY19]], 0, implicit $exec
  ; GISEL-NEXT:   [[V_ADDC_U32_e64_2:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_3:%[0-9]+]]:sreg_32_xm0_xexec = V_ADDC_U32_e64 [[COPY20]], [[COPY21]], killed [[V_ADD_CO_U32_e64_3]], 0, implicit $exec
  ; GISEL-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_2]], %subreg.sub0, [[V_ADDC_U32_e64_2]], %subreg.sub1
  ; GISEL-NEXT:   [[SI_IF_BREAK:%[0-9]+]]:sreg_32_xm0_xexec = SI_IF_BREAK [[V_CMP_LE_I32_e64_]], [[PHI6]], implicit-def $scc
  ; GISEL-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32 = S_ANDN2_B32 [[PHI5]], $exec_lo, implicit-def $scc
  ; GISEL-NEXT:   [[S_AND_B32_1:%[0-9]+]]:sreg_32 = S_AND_B32 $exec_lo, [[COPY14]], implicit-def $scc
  ; GISEL-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32 = S_OR_B32 [[S_ANDN2_B32_]], [[S_AND_B32_1]], implicit-def $scc
  ; GISEL-NEXT:   [[COPY22:%[0-9]+]]:sreg_32 = COPY [[S_XOR_B32_]]
  ; GISEL-NEXT:   [[S_ANDN2_B32_1:%[0-9]+]]:sreg_32 = S_ANDN2_B32 [[PHI3]], $exec_lo, implicit-def $scc
  ; GISEL-NEXT:   [[S_AND_B32_2:%[0-9]+]]:sreg_32 = S_AND_B32 $exec_lo, [[S_OR_B32_]], implicit-def $scc
  ; GISEL-NEXT:   [[S_OR_B32_1:%[0-9]+]]:sreg_32 = S_OR_B32 [[S_ANDN2_B32_1]], [[S_AND_B32_2]], implicit-def $scc
  ; GISEL-NEXT:   SI_LOOP [[SI_IF_BREAK]], %bb.5, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_BRANCH %bb.7
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.6.if.else:
  ; GISEL-NEXT:   successors: %bb.2(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[S_MOV_B32_8:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GISEL-NEXT:   [[COPY23:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_8]]
  ; GISEL-NEXT:   [[V_LSHLREV_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHLREV_B32_e64 [[COPY23]], [[V_AND_B32_e64_]], implicit $exec
  ; GISEL-NEXT:   [[COPY24:%[0-9]+]]:sreg_64_xexec_xnull = COPY [[COPY2]]
  ; GISEL-NEXT:   [[GLOBAL_LOAD_DWORD_SADDR:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR [[COPY24]], [[V_LSHLREV_B32_e64_1]], 0, 0, implicit $exec :: (load (s32) from %ir.arrayidx7, addrspace 1)
  ; GISEL-NEXT:   [[S_MOV_B32_9:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GISEL-NEXT:   [[COPY25:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_9]]
  ; GISEL-NEXT:   [[V_CMP_GT_I32_e64_:%[0-9]+]]:sreg_32 = V_CMP_GT_I32_e64 [[GLOBAL_LOAD_DWORD_SADDR]], [[COPY25]], implicit $exec
  ; GISEL-NEXT:   [[S_ANDN2_B32_2:%[0-9]+]]:sreg_32 = S_ANDN2_B32 [[V_CMP_NE_U32_e64_]], $exec_lo, implicit-def $scc
  ; GISEL-NEXT:   [[S_AND_B32_3:%[0-9]+]]:sreg_32 = S_AND_B32 $exec_lo, [[V_CMP_GT_I32_e64_]], implicit-def $scc
  ; GISEL-NEXT:   [[S_OR_B32_2:%[0-9]+]]:sreg_32 = S_OR_B32 [[S_ANDN2_B32_2]], [[S_AND_B32_3]], implicit-def $scc
  ; GISEL-NEXT:   S_BRANCH %bb.2
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.7.Flow:
  ; GISEL-NEXT:   successors: %bb.4(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[PHI9:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_]], %bb.5
  ; GISEL-NEXT:   [[PHI10:%[0-9]+]]:sreg_32_xm0_xexec = PHI [[SI_IF_BREAK]], %bb.5
  ; GISEL-NEXT:   [[COPY26:%[0-9]+]]:sreg_32_xm0_xexec = COPY [[S_OR_B32_1]]
  ; GISEL-NEXT:   SI_END_CF [[PHI10]], implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   [[S_ANDN2_B32_3:%[0-9]+]]:sreg_32_xm0_xexec = S_ANDN2_B32 [[COPY6]], $exec_lo, implicit-def $scc
  ; GISEL-NEXT:   [[S_AND_B32_4:%[0-9]+]]:sreg_32_xm0_xexec = S_AND_B32 $exec_lo, [[COPY26]], implicit-def $scc
  ; GISEL-NEXT:   [[S_OR_B32_3:%[0-9]+]]:sreg_32_xm0_xexec = S_OR_B32 [[S_ANDN2_B32_3]], [[S_AND_B32_4]], implicit-def $scc
  ; GISEL-NEXT:   S_BRANCH %bb.4
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.8.if.end:
  ; GISEL-NEXT:   successors: %bb.9(0x40000000), %bb.10(0x40000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[SI_IF1:%[0-9]+]]:sreg_32_xm0_xexec = SI_IF [[PHI1]], %bb.10, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_BRANCH %bb.9
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.9.if.then11:
  ; GISEL-NEXT:   successors: %bb.10(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[COPY27:%[0-9]+]]:sreg_64 = COPY [[S_LOAD_DWORDX4_IMM]].sub0_sub1
  ; GISEL-NEXT:   [[COPY28:%[0-9]+]]:sreg_64 = COPY [[S_LOAD_DWORDX4_IMM]].sub2_sub3
  ; GISEL-NEXT:   [[V_MOV_B32_e32_1:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 31, implicit $exec
  ; GISEL-NEXT:   [[V_ASHRREV_I32_e64_:%[0-9]+]]:vgpr_32 = V_ASHRREV_I32_e64 [[V_MOV_B32_e32_1]], [[PHI2]], implicit $exec
  ; GISEL-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI2]], %subreg.sub0, [[V_ASHRREV_I32_e64_]], %subreg.sub1
  ; GISEL-NEXT:   [[S_MOV_B32_10:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GISEL-NEXT:   [[COPY29:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_10]]
  ; GISEL-NEXT:   [[V_LSHLREV_B64_pseudo_e64_:%[0-9]+]]:vreg_64 = V_LSHLREV_B64_pseudo_e64 [[COPY29]], [[REG_SEQUENCE3]], implicit $exec
  ; GISEL-NEXT:   [[COPY30:%[0-9]+]]:vreg_64 = COPY [[COPY28]]
  ; GISEL-NEXT:   [[COPY31:%[0-9]+]]:vgpr_32 = COPY [[COPY30]].sub0
  ; GISEL-NEXT:   [[COPY32:%[0-9]+]]:vgpr_32 = COPY [[V_LSHLREV_B64_pseudo_e64_]].sub0
  ; GISEL-NEXT:   [[COPY33:%[0-9]+]]:vgpr_32 = COPY [[COPY30]].sub1
  ; GISEL-NEXT:   [[COPY34:%[0-9]+]]:vgpr_32 = COPY [[V_LSHLREV_B64_pseudo_e64_]].sub1
  ; GISEL-NEXT:   [[V_ADD_CO_U32_e64_4:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_5:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[COPY31]], [[COPY32]], 0, implicit $exec
  ; GISEL-NEXT:   [[V_ADDC_U32_e64_4:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_5:%[0-9]+]]:sreg_32_xm0_xexec = V_ADDC_U32_e64 [[COPY33]], [[COPY34]], killed [[V_ADD_CO_U32_e64_5]], 0, implicit $exec
  ; GISEL-NEXT:   [[REG_SEQUENCE4:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_4]], %subreg.sub0, [[V_ADDC_U32_e64_4]], %subreg.sub1
  ; GISEL-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE4]], [[PHI2]], 0, 0, implicit $exec :: (store (s32) into %ir.arrayidx13, addrspace 1)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.10.if.end14:
  ; GISEL-NEXT:   SI_END_CF [[SI_IF1]], implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_ENDPGM 0
entry:
  %id = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x()
  %cmp = icmp samesign ugt i32 %id, 5
  br i1 %cmp, label %do.body, label %if.else

do.body:                                          ; preds = %entry, %do.body
  %sel.0.off0 = phi i1 [ %xor23, %do.body ], [ false, %entry ]
  %id.0 = phi i32 [ %add, %do.body ], [ %id, %entry ]
  %idxprom = zext nneg i32 %id.0 to i64
  %arrayidx = getelementptr inbounds nuw i32, ptr addrspace(1) %filter.coerce, i64 %idxprom
  %x = load i32, ptr addrspace(1) %arrayidx, align 4
  %cmp3 = icmp slt i32 %x, 7
  %xor23 = xor i1 %sel.0.off0, %cmp3
  %add = add nuw nsw i32 %id.0, 64
  %cmp5 = icmp sgt i32 %x, 11
  br i1 %cmp5, label %do.body, label %if.end

if.else:                                          ; preds = %entry
  %idxprom6 = zext nneg i32 %id to i64
  %arrayidx7 = getelementptr inbounds nuw i32, ptr addrspace(1) %filter.coerce, i64 %idxprom6
  %2 = load i32, ptr addrspace(1) %arrayidx7, align 4
  %cmp8 = icmp sgt i32 %2, 2
  br label %if.end

if.end:                                           ; preds = %do.body, %if.else
  %sel.1.in = phi i1 [ %cmp8, %if.else ], [ %xor23, %do.body ]
  %id.1 = phi i32 [ %id, %if.else ], [ %add, %do.body ]
  br i1 %sel.1.in, label %if.then11, label %if.end14

if.then11:                                        ; preds = %if.end
  %idxprom12 = sext i32 %id.1 to i64
  %arrayidx13 = getelementptr inbounds i32, ptr addrspace(1) %out.coerce, i64 %idxprom12
  store i32 %id.1, ptr addrspace(1) %arrayidx13, align 4
  br label %if.end14

if.end14:                                         ; preds = %if.then11, %if.end
  ret void
}

declare noundef i32 @llvm.amdgcn.workitem.id.x()
